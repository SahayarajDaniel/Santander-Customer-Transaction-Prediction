{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\\n\",\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "# Load scikit's random forest classifier library\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory\n",
    "os.chdir(\"E:\\EDW\\Projects\\Santander Prediction\")\n",
    "#os.getcwd()\n",
    "#'E:\\\\EDW\\\\Projects\\\\Santander Prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape  #(200000, 202)\n",
    "test_df.shape   #(200000, 201)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory data analysis:\n",
    "desc_train = train_df.describe()\n",
    "desc_test = test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Missing Value Analysis\n",
    "\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum()\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
    "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    types = []\n",
    "    for col in data.columns:\n",
    "        dtype = str(data[col].dtype)\n",
    "        types.append(dtype)\n",
    "    tt['Types'] = types\n",
    "    return(np.transpose(tt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Missing values in train dataset\n",
    "na_train = missing_data(train_df)\n",
    "na_train.max(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Missing values in test dataset\n",
    "na_test = missing_data(test_df)\n",
    "na_test.max(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Plots\n",
    "\n",
    "def plot_feature_scatter(df1, df2, features):\n",
    "    i = 0\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(5,5,figsize=(20,20))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(5,5,i)\n",
    "        plt.scatter(df1[feature], df2[feature], marker='+')\n",
    "        plt.xlabel(feature, fontsize=15)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Plot\n",
    "features = train_df.columns\n",
    "features = features.tolist()\n",
    "features = features[2:]\n",
    "feature_1 = features[0:25] #Splitting the features into smaller sets to plot correlation plot\n",
    "feature_2 = features[26:51]\n",
    "feature_3 = features[52:77]\n",
    "feature_4 = features[78:103]\n",
    "plot_feature_scatter(train_df[::20],test_df[::20], feature_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_scatter(train_df[::20],test_df[::20], feature_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_scatter(train_df[::20],test_df[::20], feature_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Correlation matrix\n",
    "corr=train_df.iloc[:,:50].corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr,cmap='Set1',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of target variable\n",
    "\n",
    "sns.countplot(train_df['target'], palette='tab10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density Plot:\n",
    "\n",
    "def plot_predictor_distribution(df1, features):\n",
    "    i = 0\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(10,10,figsize=(20,20))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(10,10,i)\n",
    "        sns.distplot(df1[feature], hist=False)\n",
    "        plt.xlabel(feature, fontsize=9)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n",
    "        plt.tick_params(axis='y', which='major', labelsize=6)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_df.columns.values[102:202]\n",
    "plot_predictor_distribution(train_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density Plot:\n",
    "\n",
    "def plot_feature_distribution(df1, df2, label1, label2, features):\n",
    "    i = 0\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(10,10,figsize=(20,20))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(10,10,i)\n",
    "        sns.distplot(df1[feature], hist=False,label=label1)\n",
    "        sns.distplot(df2[feature], hist=False,label=label2, color='r')\n",
    "        plt.xlabel(feature, fontsize=9)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n",
    "        plt.tick_params(axis='y', which='major', labelsize=6)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = train_df.loc[train_df['target'] == 0]\n",
    "t1 = train_df.loc[train_df['target'] == 1]\n",
    "features = train_df.columns.values[2:102]\n",
    "plot_feature_distribution(t0, t1, '0', '1', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_df.columns.values[103:202]\n",
    "plot_feature_distribution(t0, t1, '0', '1', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = train_df.columns.values[2:102]\n",
    "plot_feature_distribution(train_df, test_df, 'train', 'test', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Feature selection1::\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X = train_df.iloc[:,2:202]  #independent columns\n",
    "y = train_df.iloc[:,1]    #target column\n",
    "\n",
    "#apply SelectKBest class to extract top 20 best features\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=50)\n",
    "fit_1 = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit_1.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "feat_importance_1 = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "feat_importance_1.columns = ['Variable','Score']  #naming the dataframe columns\n",
    "feat_importance_1 = feat_importance_1.sort_values(by = ['Score'], ascending = False)\n",
    "print(feat_importance_1.nlargest(50,'Score'))\n",
    "\n",
    "Var_to_model_1 = feat_importance_1.Variable[0:50] #choosing first 50 variables with high importance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature Selection2:\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "X = train_df.iloc[:,2:202]  #independent columns\n",
    "\n",
    "y = train_df.iloc[:,1]    #target column\n",
    "y=y.astype('int64')\n",
    "feat_model = ExtraTreesClassifier( )\n",
    "feat_model.fit(X,y)\n",
    "print(feat_model.feature_importances_)\n",
    "feat_importance_2 = pd.DataFrame(feat_model.feature_importances_, index=X.columns)\n",
    "feat_importance_2['Variable'] = feat_importance_2.index\n",
    "feat_importance_2.columns = ['Score', 'Variable']\n",
    "feat_importance_2 = feat_importance_2.sort_values(by = ['Score'], ascending = False)\n",
    "feat_importance_2\n",
    "Var_to_model_2 = feat_importance_2.Variable[0:50]\n",
    "#dfscores = pd.DataFrame(fit_1.scores_)\n",
    "#dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "#feat_importance_1 = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "#feat_importance_1.columns = ['Variable','Score']  #naming the dataframe columns\n",
    "#print(feat_importance_1.nlargest(50,'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable selection from 2 methods:\n",
    "Var_to_model_1 = Var_to_model_1.tolist( )\n",
    "Var_to_model_2 = Var_to_model_2.tolist( )\n",
    "Var_to_model=[i for i in Var_to_model_1 if i in Var_to_model_2]\n",
    "Var_to_model #common variables in both variable selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Var_to_model)\n",
    "print(Var_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Field selection from teh original data set\n",
    "\n",
    "train_df_1 = train_df[['ID_code', 'target']]\n",
    "train_df_2 = train_df[Var_to_model]\n",
    "train_df_1 = pd.concat([train_df_1, train_df_2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1 = test_df[['ID_code']]\n",
    "test_df_2 = test_df[Var_to_model]\n",
    "test_df_1 = pd.concat([test_df_1, test_df_2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Analysis:\n",
    "\n",
    "def plot_outlier(df1, features):\n",
    "    i = 0\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(9,5,figsize=(20,20))\n",
    "\n",
    "    for var in features:\n",
    "        i += 1\n",
    "        plt.subplot(9,5,i)\n",
    "        sns.boxplot(x=df1[var])\n",
    "        plt.xlabel(var, fontsize=9)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n",
    "        plt.tick_params(axis='y', which='major', labelsize=6)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outlier(train_df_1, Var_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Outliers:\n",
    "\n",
    "\n",
    "for i in Var_to_model:\n",
    "    q75, q25 = np.percentile(train_df_1.loc[:, i], [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    min = q25 - (iqr*1.5)\n",
    "    max = q75 + (iqr*1.5)\n",
    "    print(i)\n",
    "    print(min)\n",
    "    print(max)\n",
    "\n",
    "    train_df_1 = train_df_1.drop(train_df_1[train_df_1.loc[:,i] < min].index)\n",
    "    train_df_1 = train_df_1.drop(train_df_1[train_df_1.loc[:,i] > max].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After removing outliers:\n",
    "plot_outlier(train_df_1, Var_to_model)\n",
    "train_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratified sampling\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=10, test_size=0.33, random_state= 100)\n",
    "\n",
    "for train_index, test_index in ss.split(train_df_1, train_df_1['target']):\n",
    "    train = train_df_1.iloc[train_index]\n",
    "    test = train_df_1.iloc[test_index]\n",
    "    \n",
    "#Target variable distribution after stratified sampling\n",
    "sns.countplot(train['target'], palette='tab10')\n",
    "sns.countplot(test['target'], palette='tab10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model _1\n",
    "#Logistic regression:\n",
    "\n",
    "def logistic_regression(y,X):\n",
    "    global result\n",
    "    logit_model=sm.Logit(y,X)\n",
    "    result=logit_model.fit()\n",
    "    print(result.summary2())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2:\n",
    "#Decision tree:\n",
    "\n",
    "def decision_tree_mod(X_train,y_train,X_test):\n",
    "    global y_pred, clf\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    # Train Decision Tree Classifer\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3:\n",
    "def random_forest_mod(X,y,x_test):\n",
    "    global y_test, clf\n",
    "    # Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "    clf = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "\n",
    "    # Train the Classifier to take the training features and learn how they relate\n",
    "    # to the training y (the species)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    # Apply the Classifier we trained to the test data (which, remember, it has never seen before)\n",
    "    y_test = clf.predict(x_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build confusion matrix\n",
    "\n",
    "def confusion_matrix(yy,xx):\n",
    "    CM = pd.crosstab(yy,xx)\n",
    "    global TN, FN, TP, FP\n",
    "\n",
    "    #let us save TP, TN, FP, FN\n",
    "    TN = CM.iloc[0,0]\n",
    "    FN = CM.iloc[1,0]\n",
    "    TP = CM.iloc[1,1]\n",
    "    FP = CM.iloc[0,1]\n",
    "\n",
    "    print(\"True Negative\", TN)\n",
    "    print(\"True Positive\", TP)\n",
    "    print(\"False Negative\", FN)\n",
    "    print(\"False Positive\", FP)\n",
    "\n",
    "    #check accuracy of model\n",
    "    #accuracy_score(y_test, y_pred)*100\n",
    "    print(\"Accuracy : \", ((TP+TN)*100)/(TP+TN+FP+FN))\n",
    "    \n",
    "def Metrics(TP,TN,FP,FN):\n",
    "    ACC = ((TP+TN)*100)/(TP+TN+FP+FN)\n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = FP/(FP+TN)\n",
    "    TNR = TN/(TN+FP)\n",
    "    FNR = FN/(FN+TP)\n",
    "    Precision = TP/(TP+FP)\n",
    "    F1 = 2*((Precision*TPR)/(Precision+TPR))\n",
    "    \n",
    "    print(\"Accuracy :\", ACC)\n",
    "    print(\"Sensitivity or Recall/TPR :\", TPR)\n",
    "    print(\"Specificity or TNR :\", TNR)\n",
    "    print(\"Fall-Out or FPR :\", FPR)\n",
    "    print(\"Miss rate or False Negative rate :\", FNR)\n",
    "    print(\"Precision :\" , Precision)\n",
    "    print(\"F1 Score: \" , F1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with stratified samples\n",
    "logistic_regression(train['target'],train[Var_to_model])\n",
    "#Predict test data\n",
    "   \n",
    "test['predicted_prob'] = result.predict(test[Var_to_model])\n",
    "test['PredictedVal'] = 1\n",
    "test.loc[test.predicted_prob < 0.5, 'PredictedVal'] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test['target'], test['PredictedVal'])\n",
    "Metrics(TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "X_test = test[Var_to_model]\n",
    "\n",
    "log_ROC_auc = roc_auc_score(test['target'], result.predict(X_test))\n",
    "fpr, tpr, threshold = roc_curve(test['target'], test['predicted_prob'])\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr, label = \"ROC Curve ( area = %0.2f )\" %log_ROC_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree with Stratified samples\n",
    "decision_tree_mod(train[Var_to_model], train['target'], test[Var_to_model])\n",
    "confusion_matrix(test['target'], y_pred)\n",
    "Metrics(TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "X_test = test[Var_to_model]\n",
    "\n",
    "log_ROC_auc = roc_auc_score(test['target'], clf.predict(X_test))\n",
    "fpr, tpr, threshold = roc_curve(test['target'], y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr, label = \"ROC Curve ( area = %0.2f )\" %log_ROC_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest with Stratified samples\n",
    "random_forest_mod(train[Var_to_model], train['target'], test[Var_to_model])\n",
    "confusion_matrix(test['target'], y_test)\n",
    "Metrics(TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "X_test = test[Var_to_model]\n",
    "\n",
    "log_ROC_auc = roc_auc_score(test['target'], clf.predict(X_test))\n",
    "fpr, tpr, threshold = roc_curve(test['target'], y_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr, label = \"ROC Curve ( area = %0.2f )\" %log_ROC_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling\n",
    "# Class count\n",
    "count_class_0, count_class_1 = train_df_1.target.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = train_df_1[train_df_1['target'] == 0]\n",
    "df_class_1 = train_df_1[train_df_1['target'] == 1]\n",
    "\n",
    "df_class_0_under = df_class_0.sample(count_class_1, replace = False)\n",
    "df_under = pd.concat([df_class_0_under, df_class_1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in ss.split(df_under, df_under['target']):\n",
    "    train = df_under.iloc[train_index]\n",
    "    test = df_under.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OverSampling\n",
    "# Class count\n",
    "count_class_0, count_class_1 = train_df_1.target.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = train_df_1[train_df_1['target'] == 0]\n",
    "df_class_1 = train_df_1[train_df_1['target'] == 1]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(count_class_0//2, replace = True)\n",
    "df_over = pd.concat([df_class_1_over, df_class_0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in ss.split(df_under, df_under['target']):\n",
    "    train = df_under.iloc[train_index]\n",
    "    test = df_under.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with stratified over-sampled data\n",
    "logistic_regression(train['target'],train[Var_to_model])\n",
    "#Predict test data\n",
    "   \n",
    "test['predicted_prob'] = result.predict(test[Var_to_model])\n",
    "test['PredictedVal'] = 1\n",
    "test.loc[test.predicted_prob < 0.5, 'PredictedVal'] = 0\n",
    "confusion_matrix(test['target'], test['PredictedVal'])\n",
    "Metrics(TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree with stratified over-sampled data\n",
    "decision_tree_mod(train[Var_to_model], train['target'], test[Var_to_model])\n",
    "confusion_matrix(test['target'], y_pred)\n",
    "Metrics(TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest with stratified over-sampled data\n",
    "random_forest_mod(train[Var_to_model], train['target'], test[Var_to_model])\n",
    "confusion_matrix(test['target'], y_test)\n",
    "Metrics(TP,TN,FP,FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
